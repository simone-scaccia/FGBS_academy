{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Application Documentation Template","text":"<p>Application Owner: Simone Scaccia, Gabriele Tromboni, Beatrice Giacobbe, Flavia De Rinaldis Document Version: v0.1 Reviewers: <p>This documentation follows the TechOps \u201cApplication Documentation Template\u201d for compliance and completeness. The documented application is Quiz Generator (CrewAI), a flow that: 1) collects user choices (provider, certification, topic, number of questions, type), 2) generates a template (<code>template_generator_crew</code>), 3) indexes certification material in Qdrant, 4) generates questions with rag_crew, 5) produces the final quiz with quiz_maker_crew.</p> <p>The structure replicates the chapters and subchapters of the official template. Source of the template: Application Documentation Template. </p>"},{"location":"accuracy-lifecycle/","title":"Accuracy Throughout the Lifecycle","text":"<p>(Template refs: Annex IV \u00b62(j)) </p>"},{"location":"accuracy-lifecycle/#data-accuracy","title":"Data Accuracy","text":"<ul> <li> <p>Source quality:   All questions are grounded in official Microsoft Learn documentation (AI-900 initially).   Using authoritative PDFs minimizes the risk of inaccurate or outdated content.  </p> </li> <li> <p>Preprocessing checks:   Documents are validated before ingestion (format checks, duplicates removed).   Qdrant indexing ensures consistent embedding storage.  </p> </li> </ul>"},{"location":"accuracy-lifecycle/#model-accuracy","title":"Model Accuracy","text":"<ul> <li> <p>Embeddings: <code>text-embedding-ada-002</code> provides semantic similarity for document retrieval.   Accuracy of retrieval is validated by checking that retrieved chunks align with the selected topic.  </p> </li> <li> <p>Question generation: <code>gpt-4o</code> is used for natural language question creation.   Human reviewers validate clarity, correctness, and relevance of generated questions.  </p> </li> </ul>"},{"location":"accuracy-lifecycle/#lifecycle-validation","title":"Lifecycle Validation","text":"<ul> <li> <p>Initial validation:   Each new certification dataset is tested by generating a small quiz (smoke test).   Outputs are checked manually for accuracy and coverage.  </p> </li> <li> <p>Ongoing validation:   Regular re-checks whenever documentation is updated or dependencies are upgraded.  </p> </li> <li> <p>Regression testing:   If quiz generation degrades after an update, previous Flow versions and datasets can be rolled back.  </p> </li> </ul>"},{"location":"accuracy-lifecycle/#user-feedback-loop","title":"User Feedback Loop","text":"<ul> <li> <p>Trainer review:   Trainers and subject-matter experts review quiz quality and flag incorrect or misleading questions.  </p> </li> <li> <p>Continuous improvement:   Feedback is incorporated into dataset updates and Flow refinements.   This ensures that accuracy is maintained throughout the lifecycle of the application.  </p> </li> </ul>"},{"location":"application-functionality/","title":"Application Functionality","text":"<p>(Template refs: Article 11; Annex IV \u00b61\u20133; Article 13)</p>"},{"location":"application-functionality/#instructions-for-use-deployers","title":"Instructions for Use (Deployers)","text":"<ul> <li>Setup: </li> <li>Create a Python virtual environment (<code>python -m venv .venv</code>) and activate it.  </li> <li>Install dependencies including CrewAI (<code>pip install crewai</code>).  </li> <li>Ensure Qdrant is running locally via Docker (<code>http://localhost:6333</code>).  </li> <li> <p>Provide LLM and Embedding configuration in a <code>.env</code> file.  </p> </li> <li> <p>Run: </p> </li> <li>Start the flow: <code>bash     crewai flow quiz_generator</code> </li> <li> <p>Follow the CLI wizard to select:  </p> <ul> <li>Provider (e.g., Microsoft Azure)  </li> <li>Certification (e.g., AI-900)  </li> <li>Topic(s) of focus  </li> <li>Number of questions  </li> <li>Question type (True/False, Multiple Choice, Open-ended)  </li> </ul> </li> <li> <p>Output: </p> </li> <li>A quiz template  </li> <li>Generated questions enriched with context via RAG  </li> <li>A final assembled quiz (JSON, Markdown, and PDF formats)</li> </ul>"},{"location":"application-functionality/#model-capabilities-limitations","title":"Model Capabilities &amp; Limitations","text":"<ul> <li>Capabilities: </li> <li>Generates practice questions aligned with official certification documentation.  </li> <li>Supports multiple question types (T/F, MCQ, Open-ended).  </li> <li> <p>Uses Retrieval-Augmented Generation (RAG) for improved factual grounding.  </p> </li> <li> <p>Limitations: </p> </li> <li>Coverage is limited to the datasets indexed (official documentation PDFs).  </li> <li>Possible inaccuracies or ambiguities in generated questions.  </li> <li>No graphical user interface \u2014 interaction is CLI-only at present.  </li> </ul>"},{"location":"application-functionality/#input-data-requirements","title":"Input Data Requirements","text":"<ul> <li>Required datasets: </li> <li>Certification documentation PDFs downloaded from Microsoft Learn (or equivalent vendor).  </li> <li>Format: </li> <li>Files stored in <code>src/quiz_generator/dataset/&lt;provider&gt;/&lt;certification&gt;/</code> </li> <li>Supported formats: <code>.pdf</code>, <code>.md</code>, <code>.txt</code> (as parsed by the document loaders).  </li> </ul>"},{"location":"application-functionality/#output-explanation","title":"Output Explanation","text":"<ul> <li>Template: </li> <li> <p>Generated section headers and question placeholders based on user choices.  </p> </li> <li> <p>Generated questions: </p> </li> <li> <p>JSON representation including question text, options, and correct answers.  </p> </li> <li> <p>Final quiz: </p> </li> <li>Markdown, JSON, and PDF combining template + generated questions.  </li> <li>Ready for human review and optional distribution.  </li> </ul>"},{"location":"application-functionality/#system-architecture-overview","title":"System Architecture Overview","text":"<ul> <li>Components: </li> <li>TemplateGeneratorCrew \u2192 creates quiz structure.  </li> <li>RagCrew \u2192 retrieves relevant content from Qdrant and generates questions.  </li> <li>QuizMakerCrew \u2192 assembles final quiz output.  </li> <li>Database Utils \u2192 initializes and manages Qdrant collections.  </li> <li> <p>User Utils \u2192 handles CLI inputs and summary display.  </p> </li> <li> <p>Architecture Diagram (high-level):</p> </li> </ul> <p>```mermaid flowchart TD     A[User Input via CLI] --&gt; B[TemplateGeneratorCrew]     B --&gt; C[RagCrew (Qdrant + LLM)]     C --&gt; D[QuizMakerCrew]     D --&gt; E[Final Quiz Output: JSON/Markdown]</p>"},{"location":"cybersecurity/","title":"Cybersecurity","text":"<p>(Template refs: Annex IV \u00b62(j)) </p>"},{"location":"cybersecurity/#secret-management","title":"Secret Management","text":"<ul> <li>API Keys:   Azure OpenAI keys are stored in a local <code>.env</code> file.  </li> <li>Best practices: </li> <li><code>.env</code> is excluded from version control via <code>.gitignore</code>.  </li> <li>Keys are rotated periodically to reduce risk.  </li> <li>Extensions:   Secrets can be managed using Azure Key Vault or other enterprise-grade secret stores.  </li> </ul>"},{"location":"cybersecurity/#data-protection","title":"Data Protection","text":"<ul> <li>Dataset content:   Certification PDFs are public documentation; no personal or sensitive data is processed.  </li> <li>Logs:   Logs do not contain user secrets or raw API keys.  </li> <li>Isolation:   All data (datasets, embeddings) remains local in the developer environment or controlled VM.  </li> </ul>"},{"location":"cybersecurity/#network-security","title":"Network Security","text":"<ul> <li>Qdrant:   Runs locally in Docker (<code>localhost:6333</code> and <code>6334</code>) with no external exposure.  </li> <li>Outbound connections:   Only outbound calls are made to Azure OpenAI endpoints.  </li> <li>No public APIs exposed:   The application has no externally accessible endpoints.  </li> </ul>"},{"location":"cybersecurity/#access-control","title":"Access Control","text":"<ul> <li>User roles:   Currently single-user CLI interaction; no multi-user or shared access.  </li> <li>System access:   Restricted to the developer\u2019s machine or secured VM/container.  </li> </ul>"},{"location":"cybersecurity/#threat-mitigation","title":"Threat Mitigation","text":"<ul> <li>Common risks addressed: </li> <li>Unauthorized access: prevented by local isolation and secret management.  </li> <li>Data poisoning: reduced by only allowing official certification PDFs.  </li> <li>Denial of service: Flow fails gracefully when Qdrant or API limits are exceeded.  </li> <li>Monitoring:   Errors and failures are logged for audit and post-mortem analysis.  </li> </ul>"},{"location":"cybersecurity/#compliance-considerations","title":"Compliance Considerations","text":"<ul> <li>GDPR / Data Privacy: not applicable (no personal data processed).  </li> <li>AI Act: classified as Limited risk system; mitigations align with Annex IV guidance.  </li> </ul>"},{"location":"datasets/","title":"Datasets","text":"<p>(Template refs: Annex IV \u00b62(d)) </p>"},{"location":"datasets/#overview","title":"Overview","text":"<p>The Quiz Generator (CrewAI) relies on official certification documentation as its primary dataset. The goal is to ensure that generated quiz questions remain aligned with the \u201cskills measured\u201d defined by certification providers.</p>"},{"location":"datasets/#sources","title":"Sources","text":"<ul> <li>Certification: Azure AI Fundamentals (AI-900)  </li> <li>Source: AI-900 Skills Measured - Microsoft Learn </li> <li>Data acquisition: PDFs downloaded directly from Microsoft Learn documentation pages.  </li> <li>Format: <code>.pdf</code> (converted to text/Markdown during preprocessing).  </li> </ul>"},{"location":"datasets/#dataset-location","title":"Dataset Location","text":"<ul> <li>Local path: <code>src/quiz_generator/dataset/</code> </li> </ul>"},{"location":"deployment-plan/","title":"Deployment Plan","text":"<p>(Template refs: Annex IV \u00b62(g)) </p>"},{"location":"deployment-plan/#deployment-environments","title":"Deployment Environments","text":"<ul> <li>Development: </li> <li>Local machine with Python 3.11+  </li> <li>Qdrant running in Docker on <code>localhost:6333</code> </li> <li><code>.env</code> file for Azure OpenAI credentials  </li> <li> <p>Documentation browsed via MkDocs (<code>mkdocs serve</code>)  </p> </li> <li> <p>Staging (optional): </p> </li> <li>Virtual machine or containerized environment replicating production setup  </li> <li> <p>Used for integration testing and QA before release  </p> </li> <li> <p>Production (optional): </p> </li> <li>Could be deployed in a cloud VM or containerized service (Docker/Kubernetes)  </li> <li>Centralized Qdrant instance (managed or hosted)  </li> <li>Secure secret management (e.g., Azure Key Vault instead of local <code>.env</code>)  </li> </ul>"},{"location":"deployment-plan/#deployment-steps","title":"Deployment Steps","text":"<ol> <li>Set up environment (outside the project)    ```bash    # Create and activate a virtual environment    python -m venv .venv    # Windows    ..venv\\Scripts\\activate    # macOS/Linux    # source .venv/bin/activate</li> </ol> <p># Install CrewAI in the venv    pip install crewai</p> <p># Move into the project directory     cd quiz_generator</p> <pre><code># Launch the Flow (CrewAI will handle dependencies automatically)\ncrewai flow kickoff\n</code></pre>"},{"location":"eu-declaration-of-conformity/","title":"EU Declaration of Conformity","text":"<p>(Template refs: Annex IV \u00b62(m)) </p> <p>This application (Quiz Generator \u2013 CrewAI Flow) is classified as a Limited-risk AI system under the EU AI Act.  </p> <p>A full EU Declaration of Conformity is not applicable, as this is an academic project and not a commercial deployment.  </p> <p>The documentation aligns with the Application Documentation Template to demonstrate compliance with Annex IV principles.  </p>"},{"location":"general-information/","title":"General Information","text":"<p>(Template refs: EU AI Act Article 11; Annex IV \u00b61\u20133)</p>"},{"location":"general-information/#purpose-and-intended-use","title":"Purpose and Intended Use","text":"<ul> <li> <p>Purpose:   To generate practice quizzes for IT certifications (e.g., Azure AI-900) based on official documentation.   The application guides the user through selecting provider, certification, topic, number and type of questions, and produces a complete quiz (template + generated questions + final output).</p> </li> <li> <p>Intended Users: </p> </li> <li>Students and professionals preparing for certifications (e.g., Microsoft, AWS, etc.)  </li> <li>Trainers or instructors who want to create support quizzes for courses  </li> <li> <p>Corporate L&amp;D teams needing self-assessment tools  </p> </li> <li> <p>Key Performance Indicators (KPIs): </p> </li> <li>Coverage of certification topics (alignment with official \u201cskills measured\u201d)  </li> <li>Quality and clarity of generated questions (assessed via human review)  </li> <li> <p>User completion rate of quizzes  </p> </li> <li> <p>Limitations and Prohibited Uses: </p> </li> <li>Does not replace studying official certification resources  </li> <li>Generated questions may contain inaccuracies or ambiguities  </li> <li>Must not be used during official exams or in violation of provider policies  </li> </ul>"},{"location":"general-information/#operational-environment","title":"Operational Environment","text":"<ul> <li>Expected operational environment: Python 3.11+ on developer machines or a VM/container</li> <li>Vector DB: Qdrant (Docker) on <code>localhost:6333</code> / <code>6334</code></li> <li>LLM &amp; Embeddings: Azure OpenAI (configured via <code>.env</code>)</li> <li>Datasets: official certification PDFs stored in <code>src/quiz_generator/dataset/</code></li> </ul> <p>Project setup (CLI-based with CrewAI):</p> <p>```bash</p>"},{"location":"general-information/#1-create-the-virtual-environment-outside-the-project-folder","title":"1) Create the virtual environment OUTSIDE the project folder","text":"<p>cd  python -m venv .venv"},{"location":"general-information/#2-activate-the-venv","title":"2) Activate the venv","text":""},{"location":"general-information/#windows","title":"Windows","text":"<p>..venv\\Scripts\\activate</p>"},{"location":"general-information/#macoslinux","title":"macOS/Linux","text":"<p>source .venv/bin/activate</p>"},{"location":"general-information/#3-install-crewai-in-the-venv","title":"3) Install CrewAI in the venv","text":"<p>pip install crewai</p>"},{"location":"general-information/#4-obtain-the-project-and-move-into-the-folder","title":"4) Obtain the project and move into the folder","text":""},{"location":"general-information/#if-not-already-downloaded","title":"(if not already downloaded)","text":"<p>git clone https://github.com/simone-scaccia/FGBS_academy cd FGBS_academy/quiz_generator</p>"},{"location":"general-information/#5-start-the-flow-crewai-will-resolveinstall-the-flow-dependencies","title":"5) Start the Flow (CrewAI will resolve/install the Flow dependencies)","text":"<p>crewai flow kickoff</p> <ul> <li>User Interface: </li> <li>Command-line interface (interactive wizard via terminal)  </li> <li>Outputs: JSON and Markdown files containing generated quizzes  </li> </ul>"},{"location":"general-information/#additional-information","title":"Additional Information","text":"<ul> <li>Application Owners: Simone Scaccia, Gabriele Tromboni, Beatrice Giacobbe, Flavia De Rinaldis  </li> <li>Application Version: v0.1  </li> <li>Last Documentation Update: 2025-09-09  </li> </ul>"},{"location":"human-oversight/","title":"Human Oversight","text":"<p>(Template refs: Annex IV \u00b62(k)) </p>"},{"location":"human-oversight/#role-of-human-oversight","title":"Role of Human Oversight","text":"<ul> <li>The Quiz Generator (CrewAI) is a support tool for learning and exam preparation.  </li> <li>Humans remain in control of all outcomes: generated quizzes are not final until reviewed.  </li> <li>The system provides drafts of quizzes, but distribution to students or trainees requires human approval.  </li> </ul>"},{"location":"human-oversight/#review-process","title":"Review Process","text":"<ul> <li>Trainer/Reviewer tasks: </li> <li>Validate accuracy of generated questions.  </li> <li>Check clarity and remove ambiguities.  </li> <li>Ensure alignment with official certification \u201cskills measured\u201d.  </li> <li>Feedback loop: </li> <li>Review comments are incorporated into dataset updates and Flow refinements.  </li> </ul>"},{"location":"human-oversight/#safety-measures","title":"Safety Measures","text":"<ul> <li>Fail-safe defaults:   If errors occur (e.g., missing dataset, failed retrieval), the Flow aborts instead of producing potentially misleading quizzes.  </li> <li>Transparency:   Each quiz includes metadata about its source (provider, certification, topic, number of questions).  </li> <li>Traceability:   Errors and warnings are logged, allowing reviewers to understand why a quiz may not be complete.  </li> </ul>"},{"location":"human-oversight/#accountability","title":"Accountability","text":"<ul> <li>Final responsibility:   Human reviewers are accountable for approving or rejecting generated quizzes.  </li> <li>Non-automated use:   The tool cannot autonomously distribute content or replace official study material.  </li> <li>Governance alignment:   Oversight practices align with EU AI Act requirements for Limited-risk AI systems.  </li> </ul>"},{"location":"infra-and-env/","title":"Infrastructure and Environment Details","text":"<p>(Template refs: Annex IV \u00b62(e)) </p>"},{"location":"infra-and-env/#execution-environment","title":"Execution Environment","text":"<ul> <li>Programming language: Python 3.11+  </li> <li>Virtual environment: created with <code>python -m venv .venv</code> </li> <li>Dependencies: managed via <code>pip</code>, including:  </li> <li><code>crewai</code> \u2013 Flow orchestration and agent framework  </li> <li><code>langchain</code> \u2013 core framework for LLM applications  </li> <li><code>langchain_openai</code> \u2013 Azure OpenAI embeddings and chat models  </li> <li><code>langchain_core</code> \u2013 base types and utilities for LangChain  </li> <li><code>langchain_community</code> \u2013 community integrations (e.g., FAISS loaders, text splitters)  </li> <li><code>qdrant_client</code> \u2013 vector database client for Qdrant  </li> <li><code>python-dotenv</code> (<code>dotenv</code>) \u2013 environment variable management  </li> <li>Documentation engine: MkDocs Material (<code>mkdocs serve</code>)  </li> <li>Documentation engine: MkDocs Material (<code>mkdocs serve</code>)  </li> </ul>"},{"location":"infra-and-env/#vector-database","title":"Vector Database","text":"<ul> <li>Technology: Qdrant (running via Docker container)  </li> <li>Default ports: </li> <li>REST API \u2192 <code>http://localhost:6333</code> </li> <li>gRPC \u2192 <code>http://localhost:6334</code> </li> <li>Storage: Docker volume (<code>qdrant-storage</code>) bound to local project directory  </li> <li>Collection name: <code>outputs</code> </li> </ul>"},{"location":"infra-and-env/#cloud-services","title":"Cloud Services","text":"<ul> <li>Provider: Azure OpenAI  </li> <li>Chat model: <code>gpt-4o</code> </li> <li>Embedding model: <code>text-embedding-ada-002</code> </li> <li>Configuration: managed via <code>.env</code> file with API key and endpoint  </li> <li>Subscription: Azure (free or enterprise depending on account)  </li> </ul>"},{"location":"infra-and-env/#dataset-management","title":"Dataset Management","text":"<ul> <li>Local storage path: <code>src/quiz_generator/dataset/</code> </li> <li>Content: official certification PDFs (AI-900 initially)  </li> <li>Update process: manual download from Microsoft Learn when new versions are released  </li> </ul>"},{"location":"infra-and-env/#interfaces","title":"Interfaces","text":"<ul> <li>User interface: Command-line wizard (terminal-based)  </li> <li>Output formats: JSON, Markdown, PDF  </li> <li>Visualization: Flow plot available via <code>quiz_flow.plot()</code> </li> </ul>"},{"location":"infra-and-env/#security-access","title":"Security &amp; Access","text":"<ul> <li>Secrets management: local <code>.env</code> file (optionally extendable to Azure Key Vault)  </li> <li>Access control: local developer machine; no external API exposed except Qdrant (localhost only)  </li> <li>Network: isolated to localhost during development  </li> </ul>"},{"location":"integrations/","title":"Integration with External Systems","text":"<p>(Template refs: Annex IV \u00b62(f)) </p>"},{"location":"integrations/#overview","title":"Overview","text":"<p>The Quiz Generator (CrewAI) integrates with two main external systems: 1. Azure OpenAI \u2013 for Large Language Models (chat and embeddings). 2. Qdrant \u2013 as a vector database for semantic document retrieval.  </p> <p>These integrations ensure that generated quiz questions remain grounded in official certification content.</p>"},{"location":"integrations/#azure-openai-integration","title":"Azure OpenAI Integration","text":"<ul> <li>Purpose: </li> <li>Provides the chat model (<code>gpt-4o</code>) for template generation and question creation.  </li> <li> <p>Provides the embedding model (<code>text-embedding-ada-002</code>) for semantic indexing of certification documents.  </p> </li> <li> <p>Connection: </p> </li> <li>Configured via <code>.env</code> file containing:  <ul> <li><code>AZURE_OPENAI_API_KEY</code> </li> <li><code>AZURE_OPENAI_ENDPOINT</code> </li> <li><code>AZURE_OPENAI_API_VERSION</code> </li> </ul> </li> <li> <p>Access through the <code>langchain_openai</code> integration layer.  </p> </li> <li> <p>Data flow: </p> </li> <li>Input: prompts and questions defined by the CrewAI Flow.  </li> <li>Output: generated text (questions, templates, explanations).  </li> </ul>"},{"location":"integrations/#qdrant-integration","title":"Qdrant Integration","text":"<ul> <li>Purpose: </li> <li>Stores vector embeddings of certification documents.  </li> <li> <p>Enables semantic similarity search to retrieve context-relevant passages for question generation.  </p> </li> <li> <p>Deployment: </p> </li> <li>Runs as a Docker container locally on developer machines.  </li> <li> <p>Accessible via REST (<code>http://localhost:6333</code>) and gRPC (<code>http://localhost:6334</code>).  </p> </li> <li> <p>Collection: </p> </li> <li>Current collection name: <code>qdrant_storage</code>.  </li> <li> <p>Stores embeddings generated from AI-900 documentation.  </p> </li> <li> <p>Data flow: </p> </li> <li>Input: vector embeddings from <code>text-embedding-ada-002</code>.  </li> <li>Output: semantically relevant chunks for RAG (used by <code>rag_crew</code>).  </li> </ul>"},{"location":"integrations/#interaction-flow","title":"Interaction Flow","text":"<p>```mermaid sequenceDiagram     participant User     participant CrewAI as QuizGenerator Flow     participant Azure as Azure OpenAI     participant Qdrant as Qdrant DB</p> <pre><code>User-&gt;&gt;CrewAI: Provides quiz configuration (provider, cert, topic, etc.)\nCrewAI-&gt;&gt;Qdrant: Store/retrieve embeddings of certification docs\nCrewAI-&gt;&gt;Azure: Request embeddings (text-embedding-ada-002)\nCrewAI-&gt;&gt;Azure: Generate quiz questions (gpt-4o)\nAzure--&gt;&gt;CrewAI: Returns embeddings and generated content\nQdrant--&gt;&gt;CrewAI: Returns relevant context chunks\nCrewAI--&gt;&gt;User: Outputs final quiz (JSON, Markdown, PDF)\n</code></pre>"},{"location":"key-links/","title":"Key Links","text":"<p>This page collects the essential links (Single Source of Truth) for the Quiz Generator (CrewAI) project.</p>"},{"location":"key-links/#repository-project","title":"Repository &amp; Project","text":"<ul> <li>Code repository: simone-scaccia/FGBS_academy </li> <li>Main branch: <code>main</code> </li> </ul>"},{"location":"key-links/#documentation","title":"Documentation","text":"<ul> <li>Template structure: see Application Documentation Template </li> <li>Architecture &amp; Flow: see Application Functionality </li> <li>Models &amp; Datasets: Models \u00b7 Datasets</li> </ul>"},{"location":"key-links/#environments-infrastructure","title":"Environments &amp; Infrastructure","text":"<ul> <li>Local runtime (dev): Python 3.11+ \u00b7 <code>mkdocs serve</code> </li> <li>Local Vector DB (Qdrant): <code>http://localhost:6333</code> </li> <li>Qdrant Dashboard: <code>http://localhost:6333/dashboard</code> </li> <li>Cloud account / Subscription: Azure (certification documentation datasets)  </li> <li>Document storage: local dataset in <code>src/quiz_generator/dataset/</code></li> </ul>"},{"location":"key-links/#llm-secrets","title":"LLM &amp; Secrets","text":"<ul> <li>LLM/Embeddings provider: Azure OpenAI  </li> <li>Chat model: <code>gpt-4o</code> </li> <li>Embedding model: <code>text-embedding-ada-002</code> </li> <li>Secrets management: local <code>.env</code> file   </li> </ul>"},{"location":"key-links/#datasets-sources","title":"Datasets / Sources","text":"<ul> <li>Root dataset (local): <code>src/quiz_generator/dataset/</code> </li> <li>Supported certifications (initial): <code>AI-900</code> (Azure AI Fundamentals)  </li> <li>Official \"skills measured\" source: AI-900 Skills Measured - Microsoft Learn </li> <li>Other authorized sources: official PDFs downloaded from Azure Docs  </li> <li>Dataset update policy: manual update when Microsoft releases new versions of the PDFs</li> </ul>"},{"location":"key-links/#observability-operations","title":"Observability &amp; Operations","text":"<ul> <li>Troubleshooting index: see Incident Management</li> </ul>"},{"location":"key-links/#security-compliance","title":"Security &amp; Compliance","text":"<ul> <li>Applied standards: Standards applied </li> <li>EU Declaration of Conformity (if applicable): EU Declaration of conformity </li> </ul>"},{"location":"key-links/#contacts","title":"Contacts","text":"<ul> <li>Product / App Owners: Simone Scaccia, Gabriele Tromboni, Beatrice Giacobbe, Flavia De Rinaldis  </li> </ul>"},{"location":"lifecycle-management/","title":"Lifecycle Management","text":"<p>(Template refs: Annex IV \u00b62(h)) </p>"},{"location":"lifecycle-management/#versioning","title":"Versioning","text":"<ul> <li>Source code: managed with GitHub (<code>simone-scaccia/FGBS_academy</code>), main branch = <code>main</code>.  </li> <li>Flow versioning: each release of the Quiz Generator Flow is tagged in Git.  </li> <li>Documentation: versioned together with the source code; MkDocs site rebuilds per release.  </li> </ul>"},{"location":"lifecycle-management/#dataset-updates","title":"Dataset Updates","text":"<ul> <li>Source: official Microsoft Learn certification PDFs.  </li> <li>Update policy: </li> <li>Manually download new versions when Microsoft updates \u201cskills measured\u201d pages.  </li> <li>Re-index updated documents into Qdrant by re-running the Flow initialization step.  </li> <li>Traceability: keep previous versions of PDFs in a separate archive folder if needed.  </li> </ul>"},{"location":"lifecycle-management/#dependency-management","title":"Dependency Management","text":"<ul> <li>CrewAI dependencies: resolved automatically at <code>crewai flow kickoff</code>.  </li> <li>External libraries: updated periodically (LangChain, Qdrant client, dotenv, etc.).  </li> <li>Strategy: pin versions for stability in production, update regularly in development.  </li> </ul>"},{"location":"lifecycle-management/#monitoring-quality","title":"Monitoring &amp; Quality","text":"<ul> <li>Question quality: regularly reviewed by human experts.  </li> <li>Coverage: check that generated quizzes reflect the latest \u201cskills measured\u201d.  </li> <li>Error handling: Flow state logs errors (dataset missing, Qdrant not initialized, etc.).  </li> </ul>"},{"location":"lifecycle-management/#change-log","title":"Change Log","text":"<ul> <li>Minor changes: bug fixes, dependency bumps, documentation updates.  </li> <li>Major changes: new certification support, new question types, architectural updates.  </li> <li>Tracking: changes are logged in Git commits and optionally a <code>CHANGELOG.md</code>.  </li> </ul>"},{"location":"lifecycle-management/#decommissioning","title":"Decommissioning","text":"<ul> <li>If the project is deprecated, final documentation and last supported dataset version are archived.  </li> <li>Qdrant collections can be dropped and Docker volumes removed to free resources.  </li> </ul>"},{"location":"models/","title":"Models","text":"<p>(Template refs: Annex IV \u00b62(d)) </p>"},{"location":"models/#overview","title":"Overview","text":"<p>The Quiz Generator (CrewAI) application relies on Azure OpenAI models for both question generation and document embedding.</p>"},{"location":"models/#chat-model","title":"Chat Model","text":"<ul> <li>Model name: <code>gpt-4o</code> </li> <li>Provider: Azure OpenAI  </li> <li>Use case: </li> <li>Generates the quiz template structure.  </li> <li>Creates questions (True/False, MCQ, Open-ended) enriched with context retrieved from Qdrant.  </li> <li>Strengths: </li> <li>Supports multi-turn reasoning and context-aware question generation.  </li> <li>Handles natural language prompts effectively.  </li> <li>Limitations: </li> <li>May generate ambiguous or overly complex questions if the input dataset lacks clarity.  </li> <li>Requires human review before distribution.  </li> </ul>"},{"location":"models/#embedding-model","title":"Embedding Model","text":"<ul> <li>Model name: <code>text-embedding-ada-002</code> </li> <li>Provider: Azure OpenAI  </li> <li>Use case: </li> <li>Encodes certification documentation into vector representations.  </li> <li>Enables semantic search and retrieval within Qdrant.  </li> <li>Strengths: </li> <li>Lightweight, cost-effective embeddings.  </li> <li>Optimized for semantic similarity search.  </li> <li>Limitations: </li> <li>Cannot capture full reasoning or multi-step logic (embedding only encodes meaning).  </li> <li>Performance depends on the quality and coverage of the source documents.  </li> </ul>"},{"location":"models/#model-integration","title":"Model Integration","text":"<ul> <li>Embeddings are generated at dataset initialization and stored in Qdrant.  </li> <li>The chat model (<code>gpt-4o</code>) is invoked during quiz template generation and question creation.  </li> <li>Both models are orchestrated through CrewAI Flows to ensure a modular and scalable pipeline.</li> </ul>"},{"location":"risk-classification/","title":"Risk Classification","text":"<p>(Template refs: Annex IV \u00b62(f)) </p>"},{"location":"risk-classification/#classification-under-the-eu-ai-act","title":"Classification under the EU AI Act","text":"<p>The Quiz Generator (CrewAI Flow) is classified as a Limited-risk AI system according to the EU AI Act.  </p>"},{"location":"risk-classification/#rationale","title":"Rationale","text":"<ul> <li> <p>Not High-Risk:   The application does not fall under any of the high-risk categories listed in Annex III (e.g., biometric identification, education grading, recruitment, justice, critical infrastructures).  </p> </li> <li> <p>Not Prohibited:   The system does not perform subliminal manipulation, exploit vulnerabilities of specific groups, perform social scoring, or engage in real-time biometric surveillance.  </p> </li> <li> <p>Limited Risk:   The tool generates quizzes to support preparation for certification exams.  </p> </li> <li>Users are always aware they are interacting with AI (transparency obligation).  </li> <li>A human review step is required before quiz distribution (oversight obligation).  </li> <li> <p>Potential risks are limited to inaccurate or ambiguous questions.  </p> </li> <li> <p>Minimal Risk (not chosen):   While the system could be argued as minimal-risk (similar to chatbots), we classify it conservatively as Limited-risk because the generated content may influence learning outcomes.</p> </li> </ul>"},{"location":"risk-classification/#obligations","title":"Obligations","text":"<p>As a Limited-risk system, the main obligations are: - Transparency: clearly inform users that quiz content is AI-generated. - Human oversight: ensure quizzes are reviewed and approved before use in training or study. - Error reporting: maintain logs of errors and flows for traceability.  </p>"},{"location":"risk-mitigation-measures/","title":"Risk Mitigation Measures","text":"<p>(Template refs: Annex IV \u00b62(i)) </p>"},{"location":"risk-mitigation-measures/#preventive-measures","title":"Preventive Measures","text":"<ul> <li> <p>Grounding with official documentation:   All quiz questions are generated using Retrieval-Augmented Generation (RAG) with embeddings from official certification PDFs. This reduces the risk of hallucinated or irrelevant content.  </p> </li> <li> <p>Topic filtering:   The Flow enforces topic selection aligned with the \u201cskills measured\u201d of the chosen certification.  </p> </li> <li> <p>Input validation:   User inputs (provider, certification, number of questions, type) are validated before execution to prevent invalid configurations.  </p> </li> </ul>"},{"location":"risk-mitigation-measures/#protective-measures","title":"Protective Measures","text":"<ul> <li> <p>Human review:   Generated quizzes are intended for human review before distribution to students or trainees.  </p> </li> <li> <p>Error logging:   Flow state logs capture errors (e.g., dataset missing, Qdrant not initialized). Errors prevent progression to the next step, avoiding corrupted outputs.  </p> </li> <li> <p>Fallbacks:   If Qdrant retrieval fails, the Flow aborts gracefully with an error message instead of producing low-quality results.  </p> </li> </ul>"},{"location":"risk-mitigation-measures/#security-measures","title":"Security Measures","text":"<ul> <li> <p>Secret management:   Azure OpenAI API keys are stored in a local <code>.env</code> file (optionally integrable with Azure Key Vault).  </p> </li> <li> <p>Local isolation:   Qdrant runs in a local Docker container, minimizing exposure to external access.  </p> </li> <li> <p>Access control:   No external API is exposed by default; only local CLI interaction is possible.  </p> </li> </ul>"},{"location":"risk-mitigation-measures/#corrective-measures","title":"Corrective Measures","text":"<ul> <li> <p>Dataset reinitialization:   If errors occur due to corrupted embeddings or collection mismatch, the Qdrant collection can be dropped and re-created.  </p> </li> <li> <p>Rollback procedures:   Code and dataset versions are tracked with Git; rollback to a previous stable state is always possible.  </p> </li> <li> <p>Continuous improvement:   Feedback from users and trainers is incorporated into future versions of the Flow to reduce recurring issues.  </p> </li> </ul>"},{"location":"robustness/","title":"Robustness","text":"<p>(Template refs: Annex IV \u00b62(j)) </p>"},{"location":"robustness/#input-robustness","title":"Input Robustness","text":"<ul> <li>Validation of user input: </li> <li>Provider, certification, topic, number of questions, and question type are validated before quiz generation.  </li> <li> <p>Invalid configurations prevent the Flow from progressing to avoid producing corrupted outputs.  </p> </li> <li> <p>Edge cases: </p> </li> <li>Flow degrades gracefully with warnings instead of failing silently.  </li> </ul>"},{"location":"robustness/#system-robustness","title":"System Robustness","text":"<ul> <li> <p>Failure isolation:   Each Flow step (user input, database initialization, template generation, RAG question creation, final assembly) is isolated.   If one step fails, the process stops and logs the error, preventing propagation of corrupted data.  </p> </li> <li> <p>Qdrant reliability:   If the Qdrant collection is missing or corrupted, the Flow detects the issue and suggests reinitialization.   Errors such as \u201ccollection mismatch\u201d or \u201cempty request\u201d are handled gracefully.  </p> </li> </ul>"},{"location":"robustness/#model-robustness","title":"Model Robustness","text":"<ul> <li> <p>Embeddings:   Semantic search is resilient to slight variations in query phrasing.   Retrieval ensures coverage even if the topic wording differs from the documentation.  </p> </li> <li> <p>Question generation:   The model (<code>gpt-4o</code>) is prompted with structured templates to reduce variance and hallucination.   If context is missing, the system falls back to producing fewer questions rather than inaccurate ones.  </p> </li> </ul>"},{"location":"robustness/#operational-robustness","title":"Operational Robustness","text":"<ul> <li> <p>Logging:   Detailed logs are kept for each Flow step to allow troubleshooting and reproduction of issues.  </p> </li> <li> <p>Rollback:   If a failure occurs, code and datasets can be rolled back to previous stable versions.  </p> </li> <li> <p>Recovery:   Dropping and recreating the Qdrant collection allows recovery from index corruption.  </p> </li> </ul>"},{"location":"robustness/#continuous-testing","title":"Continuous Testing","text":"<ul> <li>Smoke tests: small quiz generations are run after updates to validate end-to-end functioning.  </li> <li>Stress tests: attempted with large input datasets to ensure the Flow remains stable.  </li> <li>User feedback: incorporated continuously to refine robustness over time.  </li> </ul>"},{"location":"standards-applied/","title":"Standards Applied","text":"<p>(Template refs: Annex IV \u00b62(m)) </p> <ul> <li>Coding standards: Python PEP 8, type hints with <code>pydantic</code>.  </li> <li>Documentation: MkDocs Material, TechOps template.  </li> <li>Version control: GitHub Flow (main branch + PR workflow).  </li> <li>Containerization: Docker (Qdrant image, volume persistence).  </li> <li>AI compliance: aligned with EU AI Act Annex IV requirements for Limited-risk systems.  </li> </ul>"},{"location":"troubleshooting/deployment/","title":"Troubleshooting AI Application Deployment","text":"<p>(Template refs: Annex IV \u00b62(l)) </p>"},{"location":"troubleshooting/deployment/#common-deployment-issues","title":"Common Deployment Issues","text":""},{"location":"troubleshooting/deployment/#1-virtual-environment-not-activated","title":"1. Virtual environment not activated","text":"<ul> <li>Symptom: <code>command not found: crewai</code> </li> <li>Cause: the <code>.venv</code> was created but not activated before running commands.  </li> <li>Mitigation:   ```bash   # Windows   ..venv\\Scripts\\activate   # Linux/macOS   source .venv/bin/activate</li> </ul>"},{"location":"troubleshooting/deployment/#2-missing-dependencies","title":"2. Missing dependencies","text":"<ul> <li>Symptom: <code>ModuleNotFoundError: No module named 'langchain_openai'</code> </li> <li>Cause: dependencies not installed correctly.  </li> <li>Mitigation: run the Flow setup inside the project:   ```bash   crewai flow kickoff</li> </ul>"},{"location":"troubleshooting/deployment/#3-qdrant-container-not-running","title":"3. Qdrant container not running","text":"<ul> <li>Symptom: <code>ConnectionError: Failed to connect to localhost:6333</code> </li> <li>Cause: Qdrant Docker container not started.  </li> <li>Mitigation:   ```bash   docker start qdrant   # or if not created yet:   docker run --name qdrant \\     -p 6333:6333 -p 6334:6334 \\     -v qdrant-storage:/qdrant/storage \\     qdrant/qdrant:latest</li> </ul>"},{"location":"troubleshooting/deployment/#4-invalid-env-configuration","title":"4. Invalid <code>.env</code> configuration","text":"<ul> <li>Symptom: <code>AuthenticationError: invalid API key</code> </li> <li>Cause: Azure OpenAI API key or endpoint not set correctly.  </li> <li>Mitigation: check that <code>.env</code> contains valid values, for example:</li> </ul> <pre><code>AZURE_OPENAI_API_KEY=xxxx\nAZURE_OPENAI_ENDPOINT=https://xxx.openai.azure.com/\nAZURE_OPENAI_API_VERSION=2024-02-01\n</code></pre>"},{"location":"troubleshooting/deployment/#5-missing-dataset","title":"5. Missing dataset","text":"<ul> <li>Symptom: <code>Error during database initialization</code> </li> <li>Cause: certification PDFs are missing in <code>src/quiz_generator/dataset/</code>.  </li> <li>Mitigation: download the official documentation PDFs from Microsoft Learn and place them in the dataset folder.  </li> </ul>"},{"location":"troubleshooting/infra-level-issues/","title":"Infrastructure-Level Issues","text":"<p>(Template refs: Annex IV \u00b62(l)) </p>"},{"location":"troubleshooting/infra-level-issues/#common-infrastructure-issues","title":"Common Infrastructure Issues","text":""},{"location":"troubleshooting/infra-level-issues/#1-docker-not-running","title":"1. Docker not running","text":"<ul> <li>Symptom: <code>Error response from daemon: Cannot connect to the Docker daemon</code> </li> <li>Cause: Docker Desktop (or the Docker daemon) is not started.  </li> <li>Mitigation: </li> <li>Start Docker Desktop (Windows/macOS).  </li> <li>On Linux, ensure the service is active: <code>bash     systemctl status docker     systemctl start docker</code></li> </ul>"},{"location":"troubleshooting/infra-level-issues/#2-qdrant-volume-corruption","title":"2. Qdrant volume corruption","text":"<ul> <li>Symptom: Flow crashes with <code>Service internal error: File exists (os error 17)</code> </li> <li>Cause: corrupted or conflicting Qdrant Docker volume.  </li> <li>Mitigation:   ```bash   docker stop qdrant   docker rm qdrant   docker volume rm qdrant-storage   docker volume create qdrant-storage   docker run --name qdrant \\     -p 6333:6333 -p 6334:6334 \\     -v qdrant-storage:/qdrant/storage \\     qdrant/qdrant:latest</li> </ul>"}]}